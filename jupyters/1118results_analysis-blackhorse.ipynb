{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4ec84d-c75c-4b48-8024-6c169014ff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„: D:\\App\\Pycharm\\1009MovieSchedulingSimulation\n",
      "WARNING:tensorflow:From D:\\App\\python\\Project\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\App\\python\\Project\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\App\\python\\Project\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "âœ… æ‰€æœ‰æ¨¡å—å·²æˆåŠŸå¯¼å…¥ï¼\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408dfc56-02db-4ced-9ded-0dc1617d8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# --- [ æ­¥éª¤ 0: ç¯å¢ƒä¸æ˜¾ç¤ºè®¾ç½® ] ---\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541aad34-47e7-44d0-83bc-1ff33b79cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. æ­£åœ¨åŠ è½½å¹¶é¢„å¤„ç†è¯„ä¼°æ•°æ® ---\n",
      "âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± 132 æ¡æ¨¡æ‹Ÿè®°å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- [ æ­¥éª¤ 1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ] ---\n",
    "print(\"\\n--- 1. æ­£åœ¨åŠ è½½å¹¶é¢„å¤„ç†è¯„ä¼°æ•°æ® ---\")\n",
    "csv_path = '../logs/full_evaluation_results.csv'\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['daily_actions'] = df['daily_actions'].apply(json.loads)\n",
    "    df['daily_incomes'] = df['daily_incomes'].apply(json.loads)\n",
    "    print(f\"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡æ¨¡æ‹Ÿè®°å½•ã€‚\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯ï¼šåœ¨è·¯å¾„ '{csv_path}' ä¸‹æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶ã€‚\")\n",
    "    print(\"ğŸ‘‰ è¯·å…ˆç¡®ä¿æ‚¨å·²ç»æˆåŠŸè¿è¡Œäº†æœ€æ–°çš„è¯„ä¼°è„šæœ¬ main.pyã€‚\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9cf61c2-488e-4000-8330-a12258daafba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "### æ ¸å¿ƒæ€§èƒ½ä¸é£é™©ç¨³å®šæ€§æŒ‡æ ‡åˆ†æ (æ‰€æœ‰æ—¥æœŸæ±‡æ€») ###\n",
      "================================================================================\n",
      "\n",
      "--- æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼šæ€§èƒ½æå‡ç™¾åˆ†æ¯” ---\n",
      "\n",
      "åœ¨ã€æ‰€æœ‰æ—¥æœŸã€‘æ±‡æ€»åœºæ™¯ä¸‹:\n",
      "  - RL (SAC) å¹³å‡æ”¶å…¥: 77.37 ä¸‡å…ƒ\n",
      "\n",
      "  --- SAC ä¸å„åŸºå‡†ç­–ç•¥å¯¹æ¯” ---\n",
      "    - ç›¸å¯¹äº æ•ˆç‡å¯å‘å¼ç­–ç•¥    (59.06 ä¸‡): +31.02%\n",
      "    - ç›¸å¯¹äº è´ªå©ªå¯å‘å¼ç­–ç•¥    (49.24 ä¸‡): +57.14%\n",
      "    - ç›¸å¯¹äº é™æ€å¯å‘å¼ç­–ç•¥    (31.06 ä¸‡): +149.08%\n",
      "\n",
      "  - æœ€ä½³åŸºå‡†ç­–ç•¥ (æ•ˆç‡å¯å‘å¼ç­–ç•¥): 59.06 ä¸‡å…ƒ\n",
      "  - SAC æ€§èƒ½æå‡ç™¾åˆ†æ¯” (ç›¸è¾ƒæœ€ä½³åŸºå‡†): 31.02%\n",
      "\n",
      "\n",
      "--- é£é™©æŒ‡æ ‡ï¼šæœ€å·®æƒ…å†µè¡¨ç° (ä¸‡å…ƒ) ---\n",
      "æ•°å€¼è¶Šå¤§ï¼ŒæŠ—é£é™©èƒ½åŠ›è¶Šå¼ºï¼š\n",
      "          æœ€å·®æƒ…å†µè¡¨ç° (ä¸‡å…ƒ)\n",
      "policy               \n",
      "RL (SAC)        18.68\n",
      "æ•ˆç‡å¯å‘å¼ç­–ç•¥         20.24\n",
      "è´ªå©ªå¯å‘å¼ç­–ç•¥         17.37\n",
      "é™æ€å¯å‘å¼ç­–ç•¥         13.36\n",
      "\n",
      "--- é£é™©æŒ‡æ ‡ï¼šå¹³å‡å¤æ™®æ¯”ç‡ ---\n",
      "æ•°å€¼è¶Šå¤§ï¼Œé£é™©è°ƒæ•´åæ”¶ç›Šè¶Šé«˜ï¼š\n",
      "          å¹³å‡å¤æ™®æ¯”ç‡\n",
      "policy          \n",
      "RL (SAC)    1.17\n",
      "æ•ˆç‡å¯å‘å¼ç­–ç•¥     1.45\n",
      "è´ªå©ªå¯å‘å¼ç­–ç•¥     1.26\n",
      "é™æ€å¯å‘å¼ç­–ç•¥     1.41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# (å‡è®¾ df å˜é‡å·²ç»ä»ä½ ä¹‹å‰çš„æ­¥éª¤åŠ è½½å®Œæ¯•)\n",
    "\n",
    "# --- [ æ­¥éª¤ 2: å®šä¹‰æ‰€æœ‰åˆ†æå‡½æ•° ] ---\n",
    "def calculate_sharpe(daily_rewards):\n",
    "    \"\"\"è®¡ç®—å¤æ™®æ¯”ç‡\"\"\"\n",
    "    # ã€ä¿®æ”¹ã€‘: ç¡®ä¿ daily_incomes è¢«æ­£ç¡®è§£æ\n",
    "    # æ£€æŸ¥ daily_rewards æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ï¼ˆä» CSV è¯»å…¥æ—¶ï¼‰\n",
    "    if isinstance(daily_rewards, str):\n",
    "        try:\n",
    "            # å‡è®¾å®ƒæ˜¯JSONå­—ç¬¦ä¸²\n",
    "            daily_rewards = pd.read_json(daily_rewards, typ='series')\n",
    "        except ValueError:\n",
    "            return np.nan # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›NaN\n",
    "            \n",
    "    daily_rewards = np.asarray(daily_rewards)\n",
    "    \n",
    "    # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦ä¸º0\n",
    "    std_dev = daily_rewards.std()\n",
    "    if std_dev == 0 or np.isnan(std_dev):\n",
    "        return 0.0\n",
    "        \n",
    "    return daily_rewards.mean() / std_dev\n",
    "\n",
    "# --- [ æ­¥éª¤ 3: è®¡ç®—å¹¶å±•ç¤ºé«˜çº§æ€§èƒ½ä¸é£é™©æŒ‡æ ‡ (å·²åˆå¹¶åœºæ™¯) ] ---\n",
    "if not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### æ ¸å¿ƒæ€§èƒ½ä¸é£é™©ç¨³å®šæ€§æŒ‡æ ‡åˆ†æ (æ‰€æœ‰æ—¥æœŸæ±‡æ€») ###\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ã€ä¿®æ”¹ã€‘: ä¸å†æŒ‰ 'scenario' åˆ†ç»„\n",
    "    summary_avg = df.groupby('policy')['total_income'].mean()\n",
    "    \n",
    "    print(\"\\n--- æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼šæ€§èƒ½æå‡ç™¾åˆ†æ¯” ---\\n\")\n",
    "    \n",
    "    sac_policy_name = 'RL (SAC)'\n",
    "    if sac_policy_name not in summary_avg.index:\n",
    "        print(f\"è­¦å‘Šï¼šåœ¨ç»“æœä¸­æ‰¾ä¸åˆ°ä¸»æ¨¡å‹ '{sac_policy_name}'ï¼Œè·³è¿‡æ€§èƒ½æå‡è®¡ç®—ã€‚\")\n",
    "    else:\n",
    "        sac_income = summary_avg[sac_policy_name]\n",
    "        \n",
    "        # æŸ¥æ‰¾åŒ…å« 'SAC' çš„æ‰€æœ‰ç­–ç•¥å\n",
    "        sac_variants_to_drop = summary_avg.index[summary_avg.index.str.contains('SAC')]\n",
    "        # æ’é™¤æ‰€æœ‰ 'SAC' å˜ä½“ï¼Œå‰©ä¸‹çš„éƒ½æ˜¯åŸºå‡†ç­–ç•¥\n",
    "        baselines_incomes = summary_avg.drop(sac_variants_to_drop)\n",
    "        \n",
    "        if not baselines_incomes.empty:\n",
    "            print(f\"åœ¨ã€æ‰€æœ‰æ—¥æœŸã€‘æ±‡æ€»åœºæ™¯ä¸‹:\")\n",
    "            print(f\"  - {sac_policy_name} å¹³å‡æ”¶å…¥: {sac_income:.2f} ä¸‡å…ƒ\")\n",
    "            \n",
    "            # ========================= ã€ã€ä¿®æ”¹ç‚¹ï¼šå¾ªç¯å¯¹æ¯”æ‰€æœ‰åŸºå‡†ã€‘ã€‘ =========================\n",
    "            print(\"\\n  --- SAC ä¸å„åŸºå‡†ç­–ç•¥å¯¹æ¯” ---\")\n",
    "            for baseline_name, baseline_income in baselines_incomes.items():\n",
    "                if baseline_income > 0:\n",
    "                    improvement_pct = (sac_income / baseline_income - 1) * 100\n",
    "                    print(f\"    - ç›¸å¯¹äº {baseline_name:<10} ({baseline_income:.2f} ä¸‡): {improvement_pct:+.2f}%\")\n",
    "                else:\n",
    "                    print(f\"    - æ— æ³•å¯¹æ¯” {baseline_name:<10} (åŸºå‡†æ”¶å…¥ä¸º0)\")\n",
    "            \n",
    "            # (ä¿ç•™æœ€ä½³åŸºå‡†çš„å¯¹æ¯”)\n",
    "            best_baseline_income = baselines_incomes.max()\n",
    "            best_baseline_name = baselines_incomes.idxmax()\n",
    "            total_improvement_pct = (sac_income / best_baseline_income - 1) * 100\n",
    "            print(f\"\\n  - æœ€ä½³åŸºå‡†ç­–ç•¥ ({best_baseline_name}): {best_baseline_income:.2f} ä¸‡å…ƒ\")\n",
    "            print(f\"  - SAC æ€§èƒ½æå‡ç™¾åˆ†æ¯” (ç›¸è¾ƒæœ€ä½³åŸºå‡†): {total_improvement_pct:.2f}%\\n\")\n",
    "            # =================================================================================\n",
    "\n",
    "        else:\n",
    "            print(f\"åœ¨ã€æ‰€æœ‰æ—¥æœŸã€‘æ±‡æ€»åœºæ™¯ä¸‹: æ²¡æœ‰æ‰¾åˆ°å¯ä¾›å¯¹æ¯”çš„åŸºå‡†ç­–ç•¥ã€‚\\n\")\n",
    "\n",
    "    # ã€ã€ä¿®æ”¹ã€‘ã€‘: ä¸å†æŒ‰ 'scenario' åˆ†ç»„\n",
    "    summary_min = df.groupby('policy')['total_income'].min().to_frame(name='æœ€å·®æƒ…å†µè¡¨ç° (ä¸‡å…ƒ)')\n",
    "    print(\"\\n--- é£é™©æŒ‡æ ‡ï¼šæœ€å·®æƒ…å†µè¡¨ç° (ä¸‡å…ƒ) ---\\næ•°å€¼è¶Šå¤§ï¼ŒæŠ—é£é™©èƒ½åŠ›è¶Šå¼ºï¼š\")\n",
    "    print(summary_min.round(2))\n",
    "\n",
    "    # ã€ã€ä¿®æ”¹ã€‘ã€‘: ç¡®ä¿ daily_incomes è¢«æ­£ç¡®è§£æ\n",
    "    # å‡è®¾ 'daily_incomes' æ˜¯ä» CSV è¯»å…¥çš„ JSON å­—ç¬¦ä¸²\n",
    "    df['sharpe_ratio'] = df['daily_incomes'].apply(calculate_sharpe)\n",
    "    \n",
    "    # ã€ã€ä¿®æ”¹ã€‘ã€‘: ä¸å†æŒ‰ 'scenario' åˆ†ç»„\n",
    "    summary_sharpe = df.groupby('policy')['sharpe_ratio'].mean().to_frame(name='å¹³å‡å¤æ™®æ¯”ç‡')\n",
    "    print(\"\\n--- é£é™©æŒ‡æ ‡ï¼šå¹³å‡å¤æ™®æ¯”ç‡ ---\\næ•°å€¼è¶Šå¤§ï¼Œé£é™©è°ƒæ•´åæ”¶ç›Šè¶Šé«˜ï¼š\")\n",
    "    print(summary_sharpe.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89be61c2-f738-4e9a-a577-67d7b94ca348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ æ£€æµ‹åˆ°äº¤äº’å¼ç¯å¢ƒï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•: D:\\App\\Pycharm\\1118MovieSchedulingSimulation\\jupyters\n",
      "ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®: D:\\App\\Pycharm\\1118MovieSchedulingSimulation\\logs\\full_evaluation_results.csv\n",
      "âœ… æˆåŠŸåŠ è½½ 132 æ¡è®°å½•ã€‚\n",
      "ğŸ“‚ å›¾ç‰‡å°†ä¿å­˜è‡³: D:\\App\\Pycharm\\1118MovieSchedulingSimulation\\jupyters\\plots\\black_horse_analysis\n",
      "\n",
      "ğŸš€ å¼€å§‹åˆ†æä¸ç»˜å›¾ï¼Œå…±å‘ç° 33 ä¸ªæ¡ˆä¾‹...\n",
      "================================================================================\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæµæµªåœ°çƒã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæ¯”æ‚²ä¼¤æ›´æ‚²ä¼¤çš„æ•…äº‹ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šåˆºæ€å°è¯´å®¶ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šäººæ½®æ±¹æ¶Œã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šç†Šå‡ºæ²¡Â·ç‹‚é‡å¤§é™†ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæˆ‘çš„å§å§ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæ‚¬å´–ä¹‹ä¸Šã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæˆ‘å’Œæˆ‘çš„çˆ¶è¾ˆã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæ‰¬åç«‹ä¸‡ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šè¿™ä¸ªæ€æ‰‹ä¸å¤ªå†·é™ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šç†Šå‡ºæ²¡Â·é‡è¿”åœ°çƒã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šç‹™å‡»æ‰‹ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå››æµ·ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šç†Šå‡ºæ²¡Â·ä¼´æˆ‘â€œç†ŠèŠ¯â€ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæ— åã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæ·±æµ·ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šäººç”Ÿè·¯ä¸ç†Ÿã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šé•¿å®‰ä¸‰ä¸‡é‡Œã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå°ç¥ç¬¬ä¸€éƒ¨ï¼šæœæ­Œé£äº‘ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå­¦çˆ¸ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šåšå¦‚ç£çŸ³ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå‰ä»»4ï¼šè‹±å¹´æ—©å©šã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå¿—æ„¿å†›ï¼šé›„å…µå‡ºå‡»ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šè«æ–¯ç§‘è¡ŒåŠ¨ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šé‡‘æ‰‹æŒ‡ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šç¬¬äºŒåæ¡ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šç†Šå‡ºæ²¡Â·é€†è½¬æ—¶ç©ºã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå‘¨å¤„é™¤ä¸‰å®³ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šä½ æƒ³æ´»å‡ºæ€æ ·çš„äººç”Ÿã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šæœ«è·¯ç‹‚èŠ±é’±ã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šä¹é¾™åŸå¯¨ä¹‹å›´åŸã€‹...\n",
      "ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Šå¼‚å½¢ï¼šå¤ºå‘½èˆ°ã€‹...\n",
      "------------------------------------------------------------\n",
      "âœ… æ‰€æœ‰æ’ç‰‡è¶‹åŠ¿å›¾å·²ä¿å­˜è‡³:\n",
      "   D:\\App\\Pycharm\\1118MovieSchedulingSimulation\\jupyters\\plots\\black_horse_analysis\n",
      "\n",
      "================================================================================\n",
      "### é»‘é©¬æ•è·é€Ÿåº¦æ±‡æ€» (è¾¾åˆ°20%æ’ç‰‡æ‰€éœ€å¤©æ•°) ###\n",
      "æ•°å€¼è¶Šå°è¶Šå¥½ï¼Œ'-' ä»£è¡¨20å¤©å†…æœªè¾¾æ ‡\n",
      "================================================================================\n",
      "policy      RL (SAC)  æ•ˆç‡å¯å‘å¼ç­–ç•¥  è´ªå©ªå¯å‘å¼ç­–ç•¥  é™æ€å¯å‘å¼ç­–ç•¥\n",
      "æµæµªåœ°çƒ             1.0      1.0      1.0        -\n",
      "æ¯”æ‚²ä¼¤æ›´æ‚²ä¼¤çš„æ•…äº‹        4.0      1.0      1.0        -\n",
      "åˆºæ€å°è¯´å®¶              -      1.0      1.0        -\n",
      "äººæ½®æ±¹æ¶Œ               -      1.0      1.0        -\n",
      "ç†Šå‡ºæ²¡Â·ç‹‚é‡å¤§é™†           -      1.0      1.0        -\n",
      "æˆ‘çš„å§å§             2.0      1.0      1.0        -\n",
      "æ‚¬å´–ä¹‹ä¸Š             1.0      1.0      1.0        -\n",
      "æˆ‘å’Œæˆ‘çš„çˆ¶è¾ˆ          16.0      1.0      1.0        -\n",
      "æ‰¬åç«‹ä¸‡             1.0      1.0      1.0        -\n",
      "è¿™ä¸ªæ€æ‰‹ä¸å¤ªå†·é™           -      1.0      1.0        -\n",
      "ç†Šå‡ºæ²¡Â·é‡è¿”åœ°çƒ         1.0      1.0      1.0        -\n",
      "ç‹™å‡»æ‰‹              1.0      1.0      1.0        -\n",
      "å››æµ·                 -      1.0      1.0        -\n",
      "ç†Šå‡ºæ²¡Â·ä¼´æˆ‘â€œç†ŠèŠ¯â€      18.0      1.0      1.0        -\n",
      "æ— å               1.0      3.0      1.0        -\n",
      "æ·±æµ·               1.0      4.0      1.0        -\n",
      "äººç”Ÿè·¯ä¸ç†Ÿ            1.0      1.0      1.0        -\n",
      "é•¿å®‰ä¸‰ä¸‡é‡Œ            9.0      1.0      1.0        -\n",
      "å°ç¥ç¬¬ä¸€éƒ¨ï¼šæœæ­Œé£äº‘       9.0      1.0      1.0        -\n",
      "å­¦çˆ¸                 -      1.0      1.0        -\n",
      "åšå¦‚ç£çŸ³             2.0      1.0      1.0        -\n",
      "å‰ä»»4ï¼šè‹±å¹´æ—©å©š         3.0      1.0      1.0        -\n",
      "å¿—æ„¿å†›ï¼šé›„å…µå‡ºå‡»         1.0      1.0      1.0        -\n",
      "è«æ–¯ç§‘è¡ŒåŠ¨              -      1.0      1.0        -\n",
      "é‡‘æ‰‹æŒ‡              3.0      1.0      1.0        -\n",
      "ç¬¬äºŒåæ¡               -      1.0      1.0        -\n",
      "ç†Šå‡ºæ²¡Â·é€†è½¬æ—¶ç©º         2.0      1.0      1.0        -\n",
      "å‘¨å¤„é™¤ä¸‰å®³            2.0      1.0      1.0        -\n",
      "ä½ æƒ³æ´»å‡ºæ€æ ·çš„äººç”Ÿ        1.0      1.0      1.0        -\n",
      "æœ«è·¯ç‹‚èŠ±é’±              -      1.0        -        -\n",
      "ä¹é¾™åŸå¯¨ä¹‹å›´åŸ            -      1.0      1.0        -\n",
      "å¼‚å½¢ï¼šå¤ºå‘½èˆ°             -      1.0      1.0        -\n",
      "\n",
      "=== å¹³å‡æ•è·å¤©æ•° (è¶Šå°è¶Šå¥½) ===\n",
      "policy\n",
      "è´ªå©ªå¯å‘å¼ç­–ç•¥    1.00\n",
      "æ•ˆç‡å¯å‘å¼ç­–ç•¥    1.16\n",
      "RL (SAC)   3.81\n",
      "é™æ€å¯å‘å¼ç­–ç•¥     NaN\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾å­—ä½“ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç \n",
    "# Windowsé€šå¸¸ä½¿ç”¨ SimHei æˆ– Microsoft YaHei\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. æ•°æ®åŠ è½½å‡½æ•°\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_data(csv_path='logs/full_evaluation_results.csv'):\n",
    "    \"\"\"è¯»å–CSVå¹¶è§£æJSONåˆ—\"\"\"\n",
    "    \n",
    "    # --- [å…³é”®ä¿®å¤] æ™ºèƒ½å¯»æ‰¾ç»å¯¹è·¯å¾„ (å…¼å®¹ Jupyter/æ§åˆ¶å°) ---\n",
    "    try:\n",
    "        # æ­£å¸¸è„šæœ¬è¿è¡Œæ—¶ä½¿ç”¨ __file__\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # Jupyter Notebook æˆ– Python Console ä¸­ __file__ æœªå®šä¹‰\n",
    "        # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ä»£æ›¿\n",
    "        current_dir = os.getcwd()\n",
    "        print(f\"âš ï¸ æ£€æµ‹åˆ°äº¤äº’å¼ç¯å¢ƒï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•: {current_dir}\")\n",
    "\n",
    "    # å‡è®¾é¡¹ç›®ç»“æ„æ˜¯ standard çš„ï¼Œå‘ä¸Šæ‰¾æˆ–è€…å°±åœ¨å½“å‰\n",
    "    possible_paths = [\n",
    "        csv_path, # ç›¸å¯¹è·¯å¾„\n",
    "        os.path.join(current_dir, csv_path), # ç»å¯¹è·¯å¾„1\n",
    "        os.path.join(current_dir, 'logs', 'full_evaluation_results.csv'), # ç»å¯¹è·¯å¾„2\n",
    "        os.path.join(os.path.dirname(current_dir), 'logs', 'full_evaluation_results.csv') # çˆ¶çº§ç›®å½•çš„logs\n",
    "    ]\n",
    "    \n",
    "    found_path = None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            found_path = p\n",
    "            break\n",
    "            \n",
    "    if not found_path:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šåœ¨ä»¥ä¸‹è·¯å¾„å‡æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶:\\n{possible_paths}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®: {found_path}\")\n",
    "\n",
    "    # è¯»å– CSV (å¸¦ç¼–ç å®¹é”™)\n",
    "    try:\n",
    "        df = pd.read_csv(found_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(found_path, encoding='gbk')\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¯»å–å¤±è´¥: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # è§£æ JSON å­—ç¬¦ä¸²\n",
    "    for col in ['daily_actions', 'daily_incomes']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # ç¡®ä¿ç´¢å¼•æ˜¯æ•°å€¼\n",
    "    if 'case_study_index' in df.columns:\n",
    "        df['case_study_index'] = pd.to_numeric(df['case_study_index'], errors='coerce')\n",
    "\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•ã€‚\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. æŒ‡æ ‡è®¡ç®—å‡½æ•°\n",
    "# -----------------------------------------------------------------------------\n",
    "def calculate_capture_speed(daily_actions, movie_index, threshold=0.20, max_days=20):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ•è·é€Ÿåº¦ï¼šç¬¬å‡ å¤©æ’ç‰‡å æ¯”é¦–æ¬¡ >= threshold\n",
    "    \"\"\"\n",
    "    if pd.isna(movie_index): return np.nan\n",
    "    movie_index = int(movie_index)\n",
    "    \n",
    "    actions = daily_actions[:max_days]\n",
    "    for day, schedule in enumerate(actions):\n",
    "        if len(schedule) > movie_index:\n",
    "            if schedule[movie_index] >= threshold:\n",
    "                return day + 1 \n",
    "    return np.nan\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. ä¸»åˆ†æé€»è¾‘\n",
    "# -----------------------------------------------------------------------------\n",
    "def analyze_and_plot():\n",
    "    df = load_data()\n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾ã€‚è¯·å…ˆè¿è¡Œ main.py ç”Ÿæˆæ•°æ®ã€‚\")\n",
    "        return\n",
    "\n",
    "    # --- [å…³é”®ä¿®å¤] ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨å¹¶ä½¿ç”¨ç»å¯¹è·¯å¾„ ---\n",
    "    try:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "    plots_dir = os.path.join(current_dir, 'plots', 'black_horse_analysis')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    print(f\"ğŸ“‚ å›¾ç‰‡å°†ä¿å­˜è‡³: {plots_dir}\")\n",
    "    \n",
    "    # ========================= [ 34éƒ¨ é»‘é©¬ç”µå½±å®Œæ•´åå• ] =========================\n",
    "    DARK_HORSE_MOVIES = [\n",
    "        'æµæµªåœ°çƒ', 'ç†Šå‡ºæ²¡Â·åŸå§‹æ—¶ä»£', 'æ–°å–œå‰§ä¹‹ç‹',\n",
    "        'åˆºæ€å°è¯´å®¶', 'äººæ½®æ±¹æ¶Œ', 'ç†Šå‡ºæ²¡Â·ç‹‚é‡å¤§é™†',\n",
    "        'è¿™ä¸ªæ€æ‰‹ä¸å¤ªå†·é™', 'ç†Šå‡ºæ²¡Â·é‡è¿”åœ°çƒ', 'ç‹™å‡»æ‰‹', 'å››æµ·',\n",
    "        'ç†Šå‡ºæ²¡Â·ä¼´æˆ‘â€œç†ŠèŠ¯â€', 'æ— å', 'æ·±æµ·', 'ç¬¬äºŒåæ¡', 'ç†Šå‡ºæ²¡Â·é€†è½¬æ—¶ç©º',\n",
    "        'æˆ‘å’Œæˆ‘çš„çˆ¶è¾ˆ', 'åšå¦‚ç£çŸ³', 'å‰ä»»4ï¼šè‹±å¹´æ—©å©š', 'å¿—æ„¿å†›ï¼šé›„å…µå‡ºå‡»', 'è«æ–¯ç§‘è¡ŒåŠ¨',\n",
    "        'æ‚¬å´–ä¹‹ä¸Š', 'äººç”Ÿè·¯ä¸ç†Ÿ', 'æœ«è·¯ç‹‚èŠ±é’±', 'ä¹é¾™åŸå¯¨ä¹‹å›´åŸ',\n",
    "        'å°ç¥ç¬¬ä¸€éƒ¨ï¼šæœæ­Œé£äº‘', 'é•¿å®‰ä¸‰ä¸‡é‡Œ', 'å­¦çˆ¸', 'å¼‚å½¢ï¼šå¤ºå‘½èˆ°',\n",
    "        'æ¯”æ‚²ä¼¤æ›´æ‚²ä¼¤çš„æ•…äº‹', 'æˆ‘çš„å§å§', 'æ‰¬åç«‹ä¸‡', 'é‡‘æ‰‹æŒ‡', 'å‘¨å¤„é™¤ä¸‰å®³', 'ä½ æƒ³æ´»å‡ºæ€æ ·çš„äººç”Ÿ'\n",
    "    ]\n",
    "    # =========================================================================\n",
    "\n",
    "    unique_cases = df[['start_date', 'case_study_movie']].drop_duplicates()\n",
    "    all_results = []\n",
    "\n",
    "    print(f\"\\nğŸš€ å¼€å§‹åˆ†æä¸ç»˜å›¾ï¼Œå…±å‘ç° {len(unique_cases)} ä¸ªæ¡ˆä¾‹...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # å¼€å¯äº¤äº’æ¨¡å¼ï¼Œå°è¯•è®©å›¾ç‰‡æ˜¾ç¤ºå‡ºæ¥\n",
    "    plt.ion() \n",
    "\n",
    "    for _, row in unique_cases.iterrows():\n",
    "        date = row['start_date']\n",
    "        movie = row['case_study_movie']\n",
    "        \n",
    "        if movie not in DARK_HORSE_MOVIES:\n",
    "            continue\n",
    "            \n",
    "        case_df = df[(df['start_date'] == date) & (df['case_study_movie'] == movie)].copy()\n",
    "        \n",
    "        if case_df.empty:\n",
    "            continue\n",
    "            \n",
    "        print(f\"ğŸ¨ æ­£åœ¨ç»˜å›¾: ã€Š{movie}ã€‹...\")\n",
    "\n",
    "        # --- 1. è®¡ç®—æ•è·é€Ÿåº¦ ---\n",
    "        metric_name = \"capture_speed_20d\"\n",
    "        threshold = 0.20 \n",
    "        \n",
    "        case_df[metric_name] = case_df.apply(\n",
    "            lambda r: calculate_capture_speed(r['daily_actions'], r['case_study_index'], threshold), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        summary = case_df.groupby('policy')[metric_name].mean()\n",
    "        summary.name = movie\n",
    "        all_results.append(summary)\n",
    "\n",
    "        # --- 2. ç»˜å›¾ ---\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        policies = case_df['policy'].unique()\n",
    "        styles = ['-', '--', '-.', ':']\n",
    "        \n",
    "        for i, policy in enumerate(policies):\n",
    "            run = case_df[case_df['policy'] == policy].iloc[0]\n",
    "            idx = run['case_study_index']\n",
    "            \n",
    "            if pd.isna(idx): continue\n",
    "            idx = int(idx)\n",
    "            \n",
    "            actions = run['daily_actions']\n",
    "            if not actions or len(actions) == 0: continue\n",
    "            \n",
    "            y_values = []\n",
    "            for day_act in actions[:20]:\n",
    "                if len(day_act) > idx:\n",
    "                    y_values.append(day_act[idx] * 100) \n",
    "                else:\n",
    "                    y_values.append(0)\n",
    "            \n",
    "            x_values = range(1, len(y_values) + 1)\n",
    "            \n",
    "            if \"SAC\" in policy:\n",
    "                c = '#d62728' # çº¢è‰²\n",
    "                ls = '-'      # å®çº¿\n",
    "                lw = 3.5      # åŠ ç²—\n",
    "                alpha = 1.0\n",
    "                zorder = 10   \n",
    "            else:\n",
    "                c = None      \n",
    "                ls = styles[i % len(styles)]\n",
    "                lw = 1.5\n",
    "                alpha = 0.7\n",
    "                zorder = 2\n",
    "            \n",
    "            ax.plot(x_values, y_values, label=policy, color=c, linestyle=ls, linewidth=lw, alpha=alpha, zorder=zorder)\n",
    "\n",
    "        ax.axhline(y=threshold*100, color='gray', linestyle='--', alpha=0.5, label=f'æ•è·é˜ˆå€¼ {threshold*100:.0f}%')\n",
    "        \n",
    "        ax.set_title(f\"é»‘é©¬ç”µå½±ã€Š{movie}ã€‹æ’ç‰‡è¶‹åŠ¿å“åº”æ›²çº¿\", fontsize=16)\n",
    "        ax.set_xlabel(\"ä¸Šæ˜ å¤©æ•° (Day)\", fontsize=12)\n",
    "        ax.set_ylabel(\"æ’ç‰‡å æ¯” (%)\", fontsize=12)\n",
    "        ax.set_xlim(1, 20)\n",
    "        ax.set_xticks(range(1, 21))\n",
    "        ax.set_ylim(bottom=-2)\n",
    "        \n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        ax.legend(loc='upper left')\n",
    "        \n",
    "        # --- ä¿å­˜å›¾ç‰‡ ---\n",
    "        safe_name = movie.replace('Â·', '').replace('ï¼š', '').replace(':', '').replace(' ', '')\n",
    "        filename = f\"{safe_name}_æ’ç‰‡è¶‹åŠ¿.png\"\n",
    "        save_path = os.path.join(plots_dir, filename)\n",
    "        \n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # å…³é—­å›¾å½¢ï¼Œé¿å…å¼¹çª—è¿‡å¤šå¡æ­»\n",
    "        plt.close(fig) \n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"âœ… æ‰€æœ‰æ’ç‰‡è¶‹åŠ¿å›¾å·²ä¿å­˜è‡³:\\n   {plots_dir}\")\n",
    "    \n",
    "    # --- 3. æ‰“å°æœ€ç»ˆé€Ÿåº¦æ±‡æ€»è¡¨ ---\n",
    "    if all_results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"### é»‘é©¬æ•è·é€Ÿåº¦æ±‡æ€» (è¾¾åˆ°20%æ’ç‰‡æ‰€éœ€å¤©æ•°) ###\")\n",
    "        print(\"æ•°å€¼è¶Šå°è¶Šå¥½ï¼Œ'-' ä»£è¡¨20å¤©å†…æœªè¾¾æ ‡\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        res_df = pd.concat(all_results, axis=1)\n",
    "        \n",
    "        # æ’åºç­–ç•¥\n",
    "        preferred_order = [\"RL (SAC)\", \"æ•ˆç‡å¯å‘å¼ç­–ç•¥\", \"è´ªå©ªå¯å‘å¼ç­–ç•¥\", \"é™æ€å¯å‘å¼ç­–ç•¥\"]\n",
    "        existing_order = [p for p in preferred_order if p in res_df.index]\n",
    "        remaining = [p for p in res_df.index if p not in existing_order]\n",
    "        final_order = existing_order + remaining\n",
    "        \n",
    "        res_df = res_df.loc[final_order]\n",
    "\n",
    "        print(res_df.T.to_string(float_format=\"{:.1f}\".format, na_rep=\"-\"))\n",
    "        \n",
    "        print(\"\\n=== å¹³å‡æ•è·å¤©æ•° (è¶Šå°è¶Šå¥½) ===\")\n",
    "        print(res_df.mean(axis=1).sort_values().to_string(float_format=\"{:.2f}\".format))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_and_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ce662-a6bc-41f0-bcd7-b0b2c2cfe723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06689c5b-242b-48ef-a539-3fa5f09dbeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d161f27-92d4-46e9-ac86-e9aac184c946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cc9f2-a8cd-4f43-a771-d086013b0936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a02289-7b9d-46b6-b445-b3116e432165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
